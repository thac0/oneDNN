<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN: Data Types</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library (oneDNN)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library (oneDNN)</div>
    </a>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
   <div id="projectnumber">2.1.3</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('dev_guide_data_types.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(10)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Data Types </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>oneDNN functionality supports a number of numerical data types. IEEE single precision floating point (fp32) is considered to be the golden standard in deep learning applications and is supported in all the library functions. The purpose of low precision data types support is to improve performance of compute intensive operations, such as convolutions, inner product, and recurrent neural network cells in comparison to fp32.</p>
<table class="doxtable">
<tr>
<th align="left">Data type </th><th align="left">Description  </th></tr>
<tr>
<td align="left">f32 </td><td align="left"><a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format#IEEE_754_single-precision_binary_floating-point_format:_binary32">IEEE single precision floating point</a> </td></tr>
<tr>
<td align="left">bf16 </td><td align="left"><a href="https://software.intel.com/content/www/us/en/develop/download/bfloat16-hardware-numerics-definition.html">non-IEEE 16-bit floating point</a> </td></tr>
<tr>
<td align="left">f16 </td><td align="left"><a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format#IEEE_754_half-precision_binary_floating-point_format:_binary16">IEEE half precision floating point</a> </td></tr>
<tr>
<td align="left">s8/u8 </td><td align="left">signed/unsigned 8-bit integer </td></tr>
</table>
<h2>Inference and Training</h2>
<p>oneDNN supports training and inference with the following data types:</p>
<table class="doxtable">
<tr>
<th align="left">Usage mode </th><th align="left">CPU </th><th align="left">GPU  </th></tr>
<tr>
<td align="left">Inference </td><td align="left">f32, bf16, s8/u8 </td><td align="left">f32, bf16, f16, s8/u8 </td></tr>
<tr>
<td align="left">Training </td><td align="left">f32, bf16 </td><td align="left">f32, bf16 </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>Using lower precision arithmetic may require changes in the deep learning model implementation.</dd></dl>
<p>See topics for the corresponding data types details:</p>
<ul>
<li><a class="el" href="dev_guide_inference_int8.html">Int8 Inference</a><ul>
<li><a class="el" href="dev_guide_attributes_quantization.html">Primitive Attributes: Quantization</a></li>
</ul>
</li>
<li><a class="el" href="dev_guide_training_bf16.html">Bfloat16 Training</a></li>
</ul>
<p>Individual primitives may have additional limitations with respect to data type support based on the precision requirements. The list of data types supported by each primitive is included in the corresponding sections of the developer guide.</p>
<h2>Hardware Limitations</h2>
<p>While all the platforms oneDNN supports have hardware acceleration for fp32 arithmetics, that is not the case for other data types. Support for low precision data types may not be available for older platforms. The next sections explain limitations that exist for low precision data types for Intel(R) Architecture processors, Intel Processor Graphics and Xe architecture-based Graphics.</p>
<h3>Intel(R) Architecture Processors</h3>
<p>oneDNN performance optimizations for Intel Architecture Processors are specialized based on Instruction Set Architecture (ISA). The following ISA have specialized optimizations in the library:</p>
<ul>
<li>Intel Streaming SIMD Extensions 4.1 (Intel SSE4.1)</li>
<li>Intel Advanced Vector Extensions (Intel AVX)</li>
<li>Intel Advanced Vector Extensions 2 (Intel AVX2)</li>
<li>Intel Advanced Vector Extensions 512 (Intel AVX-512)</li>
<li>Intel Deep Learning Boost (Intel DL Boost)</li>
</ul>
<p>The following table indicates the minimal supported ISA for each of the data types that oneDNN recognizes. </p>
<table class="doxtable">
<tr>
<th align="left">Data type </th><th align="left">Minimal supported ISA  </th></tr>
<tr>
<td align="left">f32 </td><td align="left">Intel SSE4.1 </td></tr>
<tr>
<td align="left">s8, u8 </td><td align="left">Intel AVX2 </td></tr>
<tr>
<td align="left">bf16 </td><td align="left">Intel DL Boost with bfloat16 support </td></tr>
<tr>
<td align="left">f16 </td><td align="left">not supported </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>See <a class="el" href="dev_guide_int8_computations.html">Nuances of int8 Computations</a> in the Developer Guide for additional limitations related to int8 arithmetic.</dd>
<dd>
The library has functional bfloat16 support on processors with Intel AVX-512 Byte and Word Instructions (AVX512BW) support for validation purposes. The performance of bfloat16 primitives on platforms without hardware acceleration for bfloat16 is 3-4x lower in comparison to the same operations on the fp32 data type.</dd></dl>
<h3>Intel(R) Processor Graphics and Xe architecture-based Graphics</h3>
<p>Intel Processor Graphics provides hardware acceleration for fp32 and fp16 arithmetic. Xe architecture-based Graphics additionally provides acceleration for int8 arithmetic (both signed and unsigned). Implementations for the bf16 data type are functional only and do not currently provide performance benefits.</p>
<table class="doxtable">
<tr>
<th align="left">Data type </th><th align="left">Support level  </th></tr>
<tr>
<td align="left">f32 </td><td align="left">optimized </td></tr>
<tr>
<td align="left">bf16 </td><td align="left">functional only </td></tr>
<tr>
<td align="left">f16 </td><td align="left">optimized </td></tr>
<tr>
<td align="left">s8, u8 </td><td align="left">optimized for Xe architecture-based Graphics (code named DG1 and Tiger Lake) </td></tr>
</table>
</div></div><!-- contents -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>